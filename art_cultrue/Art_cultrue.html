<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.361">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Yiqing Hu">
<meta name="dcterms.date" content="2024-03-28">

<title>Exploring User Preferences in the Online Painting Community: A Comparative Analysis of Human-Created and AI-Generated Artwork on Pixiv.net</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="Art_cultrue_files/libs/clipboard/clipboard.min.js"></script>
<script src="Art_cultrue_files/libs/quarto-html/quarto.js"></script>
<script src="Art_cultrue_files/libs/quarto-html/popper.min.js"></script>
<script src="Art_cultrue_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Art_cultrue_files/libs/quarto-html/anchor.min.js"></script>
<link href="Art_cultrue_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Art_cultrue_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="Art_cultrue_files/libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="Art_cultrue_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Art_cultrue_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Art_cultrue_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="Art_cultrue_files/libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">


</head>

<body>

<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Exploring User Preferences in the Online Painting Community: A Comparative Analysis of Human-Created and AI-Generated Artwork on Pixiv.net</h1>
                      </div>
  </div>
    
  <div class="quarto-title-meta-author">
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-heading">Affiliation</div>
    
      <div class="quarto-title-meta-contents">
      <p class="author">Yiqing Hu </p>
    </div>
      <div class="quarto-title-meta-contents">
          <p class="affiliation">
              University of Warsaw, Poland
            </p>
        </div>
      </div>

  <div class="quarto-title-meta">

        
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 28, 2024</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="page-columns page-rows-contents page-layout-article">
<div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
  <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">Table of contents</h2>
   
  <ul class="collapse">
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction">1 Introduction</a></li>
  <li><a href="#literature-review" id="toc-literature-review" class="nav-link" data-scroll-target="#literature-review">2 Literature Review</a></li>
  <li><a href="#methodology" id="toc-methodology" class="nav-link" data-scroll-target="#methodology">3 Methodology</a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">4 Results</a></li>
  <li><a href="#conclusion-and-future-prospects" id="toc-conclusion-and-future-prospects" class="nav-link" data-scroll-target="#conclusion-and-future-prospects">5 Conclusion and Future Prospects</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
</nav>
</div>
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction">1 Introduction</h2>
<section id="research-background" class="level3">
<h3 class="anchored" data-anchor-id="research-background">1.1 Research Background</h3>
<section id="introduction-to-the-online-art-market" class="level4">
<h4 class="anchored" data-anchor-id="introduction-to-the-online-art-market">1.1.1 Introduction to the Online Art Market</h4>
<p>The evolution of the online art market is traced back to the rise of information and communication technologies, which have accelerated the globalization of art and led to the creation of new art forms and marketing strategies. Despite the failure of first-generation online art startups during the dot.com bust in 2000, belief in the internet as a future art commerce platform persisted. The current landscape is marked by a diversity of online art businesses, including online auctions, galleries, and marketplaces, each offering new ways to buy and sell art. The structure of today’s online art market encompasses both hybrid online-offline businesses and purely online enterprises, catering to a broader audience by making art more accessible and breaking down traditional barriers to art acquisition.</p>
<p>The online art market’s capacity for rapid information dissemination, its global reach, and the challenges of assessing its size due to varying statistical reports. Despite these challenges, the online art market continues to grow, with significant contributions from traditional auction houses and new online platforms. The role of social media in the online art trade and the continuous evolution of online art market players suggest a future where online platforms could dominate the art market landscape.</p>
</section>
<section id="the-current-state-of-ai-generated-art-in-the-context-of-ai-development" class="level4">
<h4 class="anchored" data-anchor-id="the-current-state-of-ai-generated-art-in-the-context-of-ai-development">1.1.2 The Current State of AI-Generated Art in the Context of AI Development</h4>
<p>In the rapidly evolving landscape of artificial intelligence (AI), the development of AI-generated art represents a fascinating intersection of technology and creativity. This domain has seen significant advancements, fueled by both theoretical research and practical applications, as AI systems gain the ability to create artworks that resonate with human aesthetic sensibilities.</p>
<p>Historically, image generation has been a focus of AI research, with efforts aimed at capturing and replicating the detailed visual information that characterizes human-made art. Traditional methods relied on attribute representation and vector encoding to mimic the visual complexity found in art. However, the advent of Deep Recurrent Attentive Writers (DRAW) marked a shift towards more realistic image generation, leveraging recurrent neural networks to produce images that more closely resemble those created by human artists.</p>
<p>The pursuit of aesthetic impressions in AI-generated art has also been a key area of exploration. Researchers have endeavored to bridge the gap between color features and descriptive fashion words, aiming to generate images that not only replicate the form but also the emotional impact of artworks. This endeavor extends to the challenge of style transfer, where convolutional neural networks (CNNs) have been employed to adapt the style of one image to another, facilitating the creation of AI art that resonates with the stylistic nuances of various artistic movements.</p>
<p>The AI Painting system represents a significant step forward in this journey, enabling the generation of paintings based on user-provided content text, aesthetic effect words, and chosen artistic genres. This system embodies the progress in AI’s capability to not only generate art but to do so with a specific aesthetic and stylistic intent, guided by human input. The integration of Generative Adversarial Networks (GANs), specifically StackGAN++, for content generation, alongside methods for aesthetic effect modification and artistic genre simulation, underscores the sophisticated approaches being developed to create AI-generated paintings that align with human artistic and aesthetic criteria.</p>
<p>Furthermore, the AI Painting system’s ability to illustrate the painting process dynamically, offering insights into the creation of the artwork, highlights an appreciation for the artistic process itself, not just the final product. This not only enhances the user’s engagement with the generated art but also deepens the system’s alignment with the traditions of artistic creation.</p>
<p>In conclusion, the development of AI-generated art, as exemplified by systems like AI Painting, stands at the forefront of AI’s incursion into the realm of creativity and aesthetics. It represents a confluence of technological innovation and artistic expression, opening new avenues for exploration in both fields. As AI continues to evolve, its role in art generation promises to challenge and expand our understanding of creativity, blurring the lines between human and machine-generated beauty.</p>
</section>
<section id="pixivs-role-in-the-online-painting-market" class="level4">
<h4 class="anchored" data-anchor-id="pixivs-role-in-the-online-painting-market">1.1.3 Pixiv’s Role in the Online Painting Market</h4>
<p>pixiv.net is a Japanese online community for artists established in Tokyo in 2007. As of 2023, Pixiv hosts over 100 million artistic submissions and receives more than 1 billion page views per month. The platform’s primary goal is to offer artists a space to showcase their illustrations and receive feedback. Unlike other platforms like Instagram, Pixiv focuses predominantly on original artworks inspired by Japanese anime and manga, excluding most forms of photography. It utilizes a comprehensive tagging system for categorizing artworks and permits the posting of explicit sexual content, employing filters for user discretion.</p>
<p>User profiles on Pixiv give a summary of the artist, including nickname, birthday, gender, location, and a brief biography. Artists can also showcase their creative environment through a dedicated profile section. Artworks submitted can include multiple images, with the artist providing titles and captions, accompanied by up to 10 tags. These tags are essential for organizing images on the site and can be managed by the community through a system that allows adding, modifying, or reporting unpleasant tags.</p>
<p>Interaction on Pixiv is facilitated through following other users, messaging, and commenting on images, fostering a community spirit. A significant policy update in late October 2022 allowed for the uploading of AI-generated images, with requirements for artists to indicate whether their submissions are human or AI-generated. This policy, along with updates in May 2023 addressing concerns from human creators, positions Pixiv as a pioneering platform for sharing AI-generated works, making it an ideal case study for understanding the impact of AI-generated content on social media ecosystems.</p>
</section>
<section id="the-position-of-ai-painting-tools-like-sd-in-digital-painting" class="level4">
<h4 class="anchored" data-anchor-id="the-position-of-ai-painting-tools-like-sd-in-digital-painting">1.1.4 The Position of AI Painting Tools Like SD in Digital Painting</h4>
<p>Stable Diffusion is a significant advancement in the field of AI-generated art, offering a blend of artificial intelligence and deep learning techniques to create detailed and high-quality images. This technology, combining stable diffusion methods with Web UI technology, introduces a novel approach to AI painting generation, making it applicable across various domains such as digital art, concept design, and game development.</p>
<p>The core of Stable Diffusion lies in its ability to simulate the gradual evolution of images through a process akin to diffusion, thereby enabling more precise control over the image’s details and textures. This method stands out by offering a higher degree of stability and controllability compared to traditional techniques, such as Convolutional Neural Networks (CNNs). The Stable Diffusion Web UI further enhances the accessibility and usability of this technology, allowing users to easily generate AI paintings by setting parameters and controls through a web interface.</p>
<p>The Stable Diffusion process involves two main phases: the forward phase, where noise is incrementally added to an image, and the backward phase, which gradually refines the image by removing noise and adding details. This process, inspired by the natural diffusion of ink in water, results in images that are both complex and rich in detail.</p>
<p>Additionally, the adoption of a latent diffusion model, which operates in a lower-dimensional latent space, addresses the computational and memory challenges associated with high-resolution image generation. This innovative approach not only reduces the computational load but also maintains the quality of the generated images.</p>
<p>Practical applications of Stable Diffusion span from text-to-image and image-to-image generation to more specialized functions like Control Net, which enhances the precision of generated poses. The technology supports the training of custom models to further improve image quality, demonstrating its versatility and potential for creative exploration.</p>
<p>Moreover, Stable Diffusion encompasses various model types, including Checkpoint and LoRA models, each with distinct features and applications. Checkpoint models, known for their comprehensive nature, allow for uninterrupted image generation, making them suitable for tasks requiring high computational resources. On the other hand, LoRA models, optimized for specific styles or attributes, offer advantages in terms of training efficiency and resource utilization.</p>
<p>In conclusion, Stable Diffusion represents a breakthrough in AI painting generation, providing a powerful tool for artists, designers, and creators to explore new horizons in digital art and beyond. Its ability to generate realistic, high-quality images through a controlled and efficient process opens up new possibilities in the realm of artificial intelligence and art creation.</p>
</section>
</section>
<section id="research-objectives" class="level3">
<h3 class="anchored" data-anchor-id="research-objectives">1.2 Research Objectives</h3>
<p>The objective of this thesis is to explore and analyze user preferences within the online painting market, specifically focusing on the comparison between human-created artwork and AI-generated artwork. This study aims to shed light on the prevailing dynamics within the digital art community, particularly on a platform as influential as pixiv.net. By examining a comprehensive dataset that spans a significant period and covers a wide array of both AI-generated and human-created artworks, this research endeavors to understand the underlying factors that influence the popularity and ranking of these artworks in the online community.</p>
<p>The central purpose of this investigation is to discern whether there exist distinct preferences among users for AI-generated versus human-created artworks and to identify the topics or themes that significantly influence these preferences. Through a meticulous analysis employing ordered probit/logit models, this thesis will quantitatively assess the impact of various topics, as identified by a deep learning model, on the rankings of images within both categories of art. This approach not only aims to pinpoint which topics are pivotal in determining the rankings but also seeks to uncover any divergent trends in user preferences between the two types of artworks.</p>
<p>By achieving a deeper understanding of user preferences, this study aspires to offer valuable insights for artists, digital content creators, and platform administrators alike. Should the analysis reveal marked preferences for certain types of artwork or specific topics, it could guide strategic decisions regarding content creation and promotion, potentially leading to enhanced engagement and satisfaction within the online art community. Conversely, if the findings indicate minimal differences in user preferences, it would suggest a direct competition between human-created and AI-generated artworks, highlighting the need for artists and AI developers to innovate continually to captivate and retain audience interest.</p>
<p>Ultimately, this thesis is driven by the goal of not only advancing academic knowledge in the field of digital art and AI but also providing practical implications for the evolving landscape of online art markets. By unraveling the intricacies of user preferences in this unique intersection of art and technology, this research hopes to contribute to the broader discourse on the impact of AI on creative industries and the future of digital artistic expression.</p>
</section>
<section id="research-innovation" class="level3">
<h3 class="anchored" data-anchor-id="research-innovation">1.3 Research Innovation</h3>
<p>The innovative aspects of this thesis reside in its novel approach to analyzing user preferences for AI-generated versus human-created artwork within the online painting community, specifically leveraging a rich dataset from pixiv.net. This study distinguishes itself through several key innovations:</p>
<ol type="1">
<li><p><strong>Integration of Deep Learning for Topic Modeling</strong>: Utilizing BERT, a state-of-the-art deep learning model, to perform topic modeling on the tags of images represents a cutting-edge approach to understanding the thematic content of both human-created and AI-generated artworks. This method allows for the extraction of nuanced topics that reflect the diverse interests of the pixiv.net community, offering a sophisticated lens through which to examine user preferences.</p></li>
<li><p><strong>Application of Ordered Choice Models</strong>: By employing both ordered logit and ordered probit models, this research adopts a rigorous econometric method to analyze the ranking data of artworks. This approach is particularly suited to addressing the ordered nature of the data, providing a robust framework for understanding the factors that influence the popularity and ranking of artworks on pixiv.net. The inclusion of topics derived from deep learning as independent variables in these models is an innovative strategy that bridges AI and econometrics.</p></li>
<li><p><strong>Comparative Analysis of AI-Generated and Human-Created Artwork</strong>: This thesis ventures into relatively uncharted territory by conducting a comparative analysis of AI-generated and human-created artwork. By systematically investigating user preferences across these two categories, the study contributes to a deeper understanding of the impact of AI on artistic creation and reception in the digital age.</p></li>
<li><p><strong>Focus on a Leading Online Art Community</strong>: Focusing the research on pixiv.net, a platform at the forefront of digital art sharing and community engagement, provides a contextually rich and relevant setting for the study. This choice ensures that the findings are grounded in the practices and preferences of a vibrant and active online art community.</p></li>
<li><p><strong>Exploration of Strategic Implications</strong>: Beyond academic contributions, this thesis aims to offer practical insights for artists, content creators, and platform administrators by identifying strategic opportunities based on user preferences. Whether highlighting areas of potential collaboration between human and AI artists or underscoring competitive dynamics, the research seeks to inform strategies for content creation, promotion, and curation within the online art market.</p></li>
</ol>
<p>In summary, the innovative aspects of this thesis lie in its methodological rigor, its application of advanced AI techniques for topic modeling, its focus on an under-explored comparison between AI-generated and human-created artworks, and its potential to inform strategic decisions in the online art market. Through these innovations, the study aims to contribute significantly to the fields of digital art, AI in artistic creation, and online community engagement.</p>
</section>
<section id="thesis-organization" class="level3">
<h3 class="anchored" data-anchor-id="thesis-organization">1.4 Thesis Organization</h3>
<p>The organization of this thesis is meticulously designed to guide the reader through a comprehensive exploration of user preferences within the online painting market, with a focus on the comparative analysis of human-created and AI-generated artwork on pixiv.net. This structure not only facilitates a logical progression of research findings and analyses but also ensures a clear and coherent presentation of the study’s objectives, methodologies, results, and implications. The thesis is organized into five chapters, each serving a specific purpose within the overall research framework:</p>
<p><strong>Chapter 1: Introduction</strong>: This chapter sets the stage for the entire thesis by outlining the research background, stating the objectives, and highlighting the innovative aspects of the study. It provides an overview of the transformative impact of generative AI models on the art market and introduces the research hypotheses. Furthermore, this chapter discusses the dataset gathered from pixiv.net and lays out the thesis’s organizational structure, ensuring readers are well-prepared for the subsequent chapters.</p>
<p><strong>Chapter 2: Literature Review</strong>: In the second chapter, a comprehensive review of existing literature is presented, covering key studies and theories relevant to AI-generated artwork, user preferences within digital art markets, and the implications of AI technologies on artistic creation. This review not only contextualizes the current research within the broader academic discourse but also identifies gaps in the literature that the study aims to address.</p>
<p><strong>Chapter 3: Methodology</strong>: In the Methodology chapter, the research design and methods used to collect, process, and analyze the data are thoroughly described. This includes a detailed explanation of the use of BERT for topic modeling on image tags from pixiv.net, outlining how this deep learning model helps in identifying distinct topics between human-created and AI-generated artworks. Additionally, the chapter discusses the employed econometric models—specifically, ordered probit and logit models—for analyzing the influence of these topics on artwork rankings.</p>
<p><strong>Chapter 4: Results</strong>: The Results chapter presents the findings from the topic modeling and econometric analysis. It offers a quantitative exploration of the rankings of AI-generated versus human-created artworks on pixiv.net, interpreting these findings in light of the research hypotheses. This chapter aims to provide insights into user preferences and the significant topics that influence these preferences within the online painting community.</p>
<p><strong>Chapter 5: Conclusion and Future Prospects</strong>: The final chapter synthesizes the findings of the study, discussing the implications of the results for artists, content creators, and platform administrators within the online painting market. It evaluates the study’s contribution to the understanding of digital art markets and AI’s role in artistic creation. Additionally, this chapter outlines the limitations of the current research and proposes avenues for future investigations, offering a vision for continued exploration in the intersection of art, technology, and user preferences.</p>
<p>Through this structured approach, the thesis aims to provide a holistic and nuanced understanding of the dynamics at play in the online painting market, contributing valuable insights into the evolving relationship between AI technologies and artistic creation.</p>
</section>
</section>
<section id="literature-review" class="level2">
<h2 class="anchored" data-anchor-id="literature-review">2 Literature Review</h2>
<section id="the-impact-of-generative-ai-on-markets" class="level3">
<h3 class="anchored" data-anchor-id="the-impact-of-generative-ai-on-markets">2.1 The impact of generative AI on markets</h3>
<section id="the-chatgpt-effect-on-ai-themed-cryptocurrencies" class="level4">
<h4 class="anchored" data-anchor-id="the-chatgpt-effect-on-ai-themed-cryptocurrencies">2.1.1 The ChatGPT Effect on AI-Themed Cryptocurrencies</h4>
<p>“The ChatGPT Effect on AI-Themed Cryptocurrencies”<span class="citation" data-cites="AIThemedCryptocurrencies"><a href="#ref-AIThemedCryptocurrencies" role="doc-biblioref">[1]</a></span> by Lennart Ante and Ender Demir investigates the impact of ChatGPT’s launch on the returns of AI-themed cryptocurrency assets. ChatGPT, unveiled by OpenAI on November 30, 2022, quickly gained significant attention, surpassing one million users within a week. Despite OpenAI CEO Sam Altman’s caution regarding its preliminary state, ChatGPT has been regarded as a breakthrough in AI, capable of stimulating digital transformation and potentially competing with Google.</p>
<p>This study focuses on whether the launch and ensuing popularity of ChatGPT acted as a positive signal for AI-themed crypto assets, resulting in abnormal price movements. By analyzing a sample of 10 AI-themed cryptocurrencies from July to December 2022, the research implements an event study methodology to observe market efficiency and price reactions.</p>
<p>Findings revealed significant abnormal returns of up to 41% over two weeks following ChatGPT’s launch, with 90% of the analyzed tokens exhibiting positive abnormal returns. This suggests that the attention ChatGPT received translated into positive market outcomes for AI-related cryptocurrencies. The study highlights the importance of public perception and media influence on financial markets, demonstrating how advancements in technology like AI can serve as quality signals that affect asset prices.</p>
<p>Moreover, the research contributes to the broader discourse on market efficiency and signaling theory, showing that cryptocurrency markets are responsive to developments in related fields like AI. The results indicate a fragmented cryptocurrency market where AI-themed tokens, despite their correlation with Bitcoin, experienced unique dynamics influenced by external factors such as technological innovations. As a tool for generative AI, ChatGPT has had a considerable impact on the cryptocurrency market. Whether this impact will spread to the art market is worthy of further study.</p>
</section>
<section id="the-cyber-turn-of-the-contemporary-art-market" class="level4">
<h4 class="anchored" data-anchor-id="the-cyber-turn-of-the-contemporary-art-market">2.1.2 The Cyber Turn of the Contemporary Art Market</h4>
<p>In “The Cyber Turn of the Contemporary Art Market,”<span class="citation" data-cites="CyberTurnArt"><a href="#ref-CyberTurnArt" role="doc-biblioref">[2]</a></span> Elena Sidorova delves into the transformative impact of digital technologies on the online art market. The third section, “Technologies Used to Expand the Online Art Market,” discusses three pivotal innovations: cryptocurrency blockchain, and artificial intelligence (AI), that are redefining the landscape of art commerce on the internet.</p>
<ul>
<li><p><strong>Cryptocurrency and Blockchain:</strong> The paper outlines the emergence of cryptocurrency and blockchain as significant disruptors in the art market. It notes the increased media attention and the hosting of conferences dedicated to exploring their impact on art transactions. Cryptocurrency, exemplified by Bitcoin, offers a decentralized exchange medium, bypassing traditional financial institutions and using cryptography for transaction security. Blockchain serves as a distributed ledger, ensuring transaction transparency and security. These technologies offer advantages such as improved authenticity verification, privacy for collectors, and traceability of art sales. The tokenization of art, enabling fractional ownership of artworks, is highlighted as a novel development. Despite the potential benefits, challenges exist, including the risk of fostering a black market and the evolving nature of art-based lending services in the context of cryptocurrency.</p></li>
<li><p><strong>Artificial Intelligence (AI):</strong> The paper describes the advent of AI in creating art and transforming market practices. It cites the sale of an AI-generated artwork by Christie’s as a landmark event, demonstrating AI’s capability to produce art that resonates with the market. Beyond creating art, AI technologies are applied in e-marketing, employing machine learning to match buyers with artworks based on their online behavior. Additionally, emerging AI technologies such as algorithms for detecting art forgeries and virtual reality (VR) and augmented reality (AR) tools for viewing artworks are transforming online art sales by enhancing customer engagement and operational efficiency.</p></li>
</ul>
<p>Sidorova’s analysis portrays these technologies not merely as tools for market expansion but as catalysts for a conceptual shift in the art market. They represent the intersection of art, technology, and commerce, creating a new paradigm where digital innovation drives market evolution. This exploration into the integration of digital technologies within the art market provides valuable insights into the future trajectory of art commerce in the digital age, emphasizing the growing importance of technological fluency in navigating this evolving landscape. My research will narrow the focus of discussion from the art market to the digital painting market.</p>
</section>
</section>
<section id="ai-versus-humans" class="level3">
<h3 class="anchored" data-anchor-id="ai-versus-humans">2.2 AI versus humans</h3>
<section id="artistic-reflection-on-artificial-intelligence-digital-painting" class="level4">
<h4 class="anchored" data-anchor-id="artistic-reflection-on-artificial-intelligence-digital-painting">2.2.1 Artistic Reflection on Artificial Intelligence Digital Painting</h4>
<p>The article “Artistic Reflection on Artificial Intelligence Digital Painting”<span class="citation" data-cites="ArtisticReflection"><a href="#ref-ArtisticReflection" role="doc-biblioref">[3]</a></span> by Xinlu Liu delves into the evolving intersection between artificial intelligence (AI) and traditional painting. The paper posits that while AI technology has revolutionized many fields, including art, it inherently lacks the diversity and humanization found in traditional painting techniques. This limitation arises from AI’s mechanical nature, which restricts its choice of painting materials and the setting of brush parameters, thereby impacting the expressiveness and emotional depth of the resulting artworks.</p>
<p>Liu argues that despite these limitations, the continuous advancement of technology will inevitably enrich the artistic content of AI-generated works over time, contributing to their eventual acceptance and appreciation within the art community. The paper underscores the importance of integrating AI with traditional painting to foster innovation in art creation and ensure the sustainable development of both technology and humanities.</p>
<p>The article outlines several key points:</p>
<ul>
<li><p><strong>Integration of AI in Art</strong>: AI provides new possibilities for artistic creation, leading to innovations in form and diversification in the development of new media art.</p></li>
<li><p><strong>Comparison with Traditional Painting</strong>: Traditional painting possesses a unique creative mode and artistic concept that AI has yet to fully replicate, particularly in terms of material media and expressive styles.</p></li>
<li><p><strong>Interrelation and Differences</strong>: While AI and traditional painting share some theoretical foundations, significant differences remain in their expressive capabilities and impact on the viewer.</p></li>
<li><p><strong>Influence on Creation</strong>: The incorporation of AI in art has not only altered materials and tools but has also transformed artists’ creative processes and conceptual approaches.</p></li>
<li><p><strong>Future Directions</strong>: The paper advocates for a balanced integration of AI technologies and traditional art forms, emphasizing the need for continuous theoretical research and industry regulation to foster the healthy development of AI in art.</p></li>
</ul>
<p>Liu suggests that while AI technology can significantly enhance artistic creativity and introduce new forms of expression, the essence and depth of traditional art should not be overlooked. The fusion of AI with traditional painting practices offers a promising avenue for the evolution of art, provided it is navigated with a thoughtful appreciation of both mediums’ unique strengths. However, Liu’s research did not point out users’ different attitudes toward AI-generated and human-created artwork, let alone analyze user preferences based on this.</p>
</section>
<section id="from-pigments-to-pixels-a-comparison-of-human-and-ai-painting" class="level4">
<h4 class="anchored" data-anchor-id="from-pigments-to-pixels-a-comparison-of-human-and-ai-painting">2.2.2 From Pigments to Pixels: A Comparison of Human and AI Painting</h4>
<p>In the exploration of the intersection between art and artificial intelligence (AI), “From Pigments to Pixels: A Comparison of Human and AI Painting”<span class="citation" data-cites="Pigments2Pixels"><a href="#ref-Pigments2Pixels" role="doc-biblioref">[4]</a></span> embarks on a nuanced analysis of AI’s role in the creative process of painting. The overarching purpose of this research is not merely to assess AI’s ability to mimic human art indistinguishably but to understand its potential to generate art that is rich in cultural significance and capable of eliciting emotional responses from viewers.</p>
<p>The comprehensive methodology was adopted to scrutinize the distinctions and perceptions surrounding AI and human-generated artworks, focusing on the theme of ‘Home Sweet Home.’ The research utilized six paintings each from an amateur artist, Sandy Lee, and AI algorithms, specifically employing the Whitening and Coloring Transforms (WCT), Gatys, and Structure-emphasized Multimodal Style Transfer (SEMST) for style conversion. The chosen artworks represented varied interpretations of ‘home,’ selected by art history and aesthetics experts to ensure thematic consistency and richness.</p>
<p>The study engaged 380 participants, with a significant portion (70%) having prior painting experience, to compare and contrast the two sets of artworks across multiple dimensions. This diverse participant pool was pivotal for examining the nuanced perceptions and emotional resonances elicited by each set of paintings, aiming to illuminate the aesthetic and emotional differentials between human and AI artistry. Its design was methodically structured to evaluate the artworks on technical, semantic, and effectual levels, incorporating a multifaceted approach to understanding art appreciation. This included assessing elements such as color, brushstroke, and line for their technical execution; emotional impact for semantic analysis; and overall affective response for understanding the artworks’ effect level. Furthermore, the study meticulously controlled for potential biases by ensuring anonymity in the presentation of the artworks and utilized online platforms for dissemination and feedback collection, adjusting for the digital medium’s influence on artistic perception.</p>
<p>The results of this study present a detailed analysis of the participant’s ability to differentiate between AI and human paintings, the emotional impact of AI paintings, and areas where AI still lags behind human creativity. A significant finding is that a large proportion of participants mistook AI paintings for human-made works, indicating the AI’s capability to mimic human artistic expression closely. However, when analyzing the emotional impact of AI paintings, specific technical aspects like color brightness, line precision, and fluency were identified as significant predictors of the affective response to the theme of “Home Sweet Home,” suggesting that while AI can replicate styles, it struggles to evoke the deeper emotional responses associated with human creativity.</p>
<p>Furthermore, the study highlights the creative limitations of AI in art, noting that AI lacks the capacity for self-awareness and holistic perspective, crucial for the depth and emotional resonance typically found in human art. This limitation was particularly evident in cases where the AI’s representation of themes diverged from human expectations, leading to a lesser emotional impact and appreciation by the viewers.</p>
<p>A significant aspect shows AI’s inability to draw upon real-life memories and emotional associations, a cornerstone in human artistic expression. This limitation becomes evident in AI’s handling of color symbolism and emotional conveyance, where AI struggles to emulate the nuanced associations humans naturally integrate into their artworks. The discussion further critiques AI’s inability to grasp the social significance of colors, a vital element in the rich tapestry of human art that goes beyond mere aesthetics to embody cultural and emotional depth.</p>
<p>Moreover, the study explores the idea that, unlike human artists who can reflect on the overall composition, considering emotional impact, symbolic meaning, and aesthetic balance, AI lacks this reflective capacity. This is because AI approaches art creation from a purely technical standpoint, devoid of the emotional and cognitive processes that guide human creativity. Consequently, while AI can mimic specific styles and techniques, it falls short of capturing the essence and depth of human artistic expression, which is deeply rooted in personal experience, cultural context, and emotional intelligence.</p>
<p>The study recommends two primary avenues for further exploration:</p>
<ul>
<li><p><strong>Gallery-based Art Viewing for Comparative Studies</strong>: Highlighting the difference in perception between viewing art in person versus online, the study suggests future research should incorporate live gallery viewings. Such settings can offer richer, more nuanced insights into the viewer’s engagement with art, potentially unveiling aspects that digital viewings might obscure.</p></li>
<li><p><strong>Semantic Recognition to Augment AI Creativity</strong>: To bridge the gap in AI’s understanding of artistry’s semantic layers, the study proposes integrating advanced semantic recognition technologies. This could enable AI to autonomously select and interpret artworks for style transfer, thereby enhancing its learning process and creative outputs.</p></li>
</ul>
<p>The juxtaposition of human and AI artistry in this study illuminates the profound gap between technical mimicry and genuine artistic expression. While AI can replicate styles and techniques, it lacks the intrinsic human capacity to imbue art with emotional depth and societal context. As AI continues to evolve, its potential to parallel human creativity remains an open question, inviting further exploration and technological advancements.</p>
</section>
<section id="airtist-or-counterfeiter-artificial-intelligence-as-devaluating-factor-on-the-art-market" class="level4">
<h4 class="anchored" data-anchor-id="airtist-or-counterfeiter-artificial-intelligence-as-devaluating-factor-on-the-art-market">2.2.3 A(I)rtist or Counterfeiter? Artificial Intelligence as (D)Evaluating Factor on the Art Market</h4>
<p>This paper, A(I)rtist or Counterfeiter? Artificial Intelligence as (D)Evaluating Factor on the Art Market<span class="citation" data-cites="AirtistOrCounterfeiter"><a href="#ref-AirtistOrCounterfeiter" role="doc-biblioref">[5]</a></span>, explores the impact of Artificial Intelligence AI on the perception and valuation of art by audiences. The central finding is that people value paintings less when they learn these were created by AI, regardless of the painting’s style or actual market value. This indicates a heuristic difference in assessing AI-made art, viewing AI not as an equal to human artists. The study suggests significant implications for art management, highlighting potential challenges in selling AI-created paintings and the contextual influence on valuation when AI and human-made art are displayed together. The research hypothesizes that the lower perceived value of AI art is mediated by perceived artistry and overall impression and that AI-made art is excluded from the category of human art, affecting the valuation of figurative versus abstract paintings differently depending on the artist’s identity (AI or human). The conclusion reinforces that authorship by AI negatively affects art’s perceived value, suggesting human-made art is seen as superior, impacting both impression and pricing in the art market.</p>
<p>The design of the research outlines a comprehensive experimental framework to investigate how the knowledge that a painting is created by Artificial Intelligence (AI) impacts its perceived value compared to those known to be created by humans. The experiment utilized a 2x2x2 factorial design to explore variations in perceived value based on the painting’s authorship (AI vs.&nbsp;human), contextual information (value of similar artworks by AI vs.&nbsp;humans), and style (figurative vs.&nbsp;abstract). The study engaged 296 participants, randomly assigned to one of eight groups, to assess the value of paintings produced by AI algorithms. The paintings were either in the style of Paul Cezanne’s “Houses at the L’Estaque” re-interpreted by CloudPainter or an abstract still-life by Ai-Da. A unique aspect of the experimental design was the introduction of a fictional artist, “Iog Wasamaru,” to uniformly represent authorship across conditions. Contextual cues were manipulated by informing participants of the price of similar artworks by either humans or AI, exploring the impact of these cues on valuation. The study measured perceived value through participants’ price estimations, alongside assessments of the artwork’s artistry and overall impression. This design aimed to elucidate not only the direct effect of authorship on perceived value but also the mediating role of perceived artistry and overall impression, thereby providing insights into the heuristic processes underlying the valuation of art in the context of AI authorship.</p>
<p>Several results were from the experiments.</p>
<ul>
<li><p><strong>Authorship Effect</strong>: The author of the painting (AI vs.&nbsp;human) had a notable impact on its perceived value. Paintings identified as created by humans were valued higher than those attributed to AI, supporting the hypothesis that AI authorship diminishes perceived value.</p></li>
<li><p><strong>Contextual Influence</strong>: The context in which a painting’s value is considered (whether compared to AI or human-created art) also significantly affects its perceived worth. Paintings were valued more when the contextual cue was the price of similar images painted by AI than by humans. This finding suggests that the knowledge of AI authorship, when used as a contextual cue, influences the valuation process. **Independency from Painting Style</p></li>
<li><p><em>Independency from Painting Style</em>: No significant differences were observed regarding the painting’s style (figurative vs.&nbsp;abstract) on its perceived value, indicating that the style does not influence the valuation as authorship and contextual cues do.</p></li>
<li><p><strong>Mediation Analysis</strong>: Further analysis, explored how the negative impact of AI authorship on a painting’s perceived value is mediated by the author’s perceived artistry and the overall impression the art piece makes on the viewers. Both factors partially mediated the relationship, but the direct effect of authorship on perceived value remained stronger.</p></li>
</ul>
<p>These findings elucidate the significant role of authorship and contextual cues in the valuation of art, emphasizing that paintings by human artists are perceived as more valuable than those by AI, regardless of the art’s style. The study highlights the nuanced ways in which information about a painting’s origin and the contextual valuation cues influence public perception and valuation of art in the AI era.</p>
<p>The authors assert that people do not regard AI as equivalent to human artists in the realm of art creation, highlighting a general tendency to value human-created art more highly. This perception suggests a broader human inclination to reserve certain domains, such as creativity, exclusively for human endeavor, aligning with previous sociological and anthropological research. The discussion further explores how contextual information influences art valuation. Paintings associated with AI-generated art context cues are perceived as more valuable, indicating a bias towards human artistry over machine creation. This phenomenon is linked to societal narratives that often cast robots and AI as threats to human uniqueness and competence, reinforcing a desire to maintain a distinct human domain in creative endeavors.</p>
<p>The authors also touch on the concept of reactance, suggesting that the devaluation of AI art might reflect a form of resistance against the encroachment of machines into traditionally human territories. Moreover, the direct and indirect impacts of the art’s authorship on its perceived value are examined, with a notable emphasis on the overwhelming importance of knowing the artist’s identity (human or AI) in determining a painting’s worth.</p>
<p>For recommendations for art management and marketing professionals, the paper advises them to be mindful of how art created by AI is presented alongside human-created art. The juxtaposition of these works could negatively influence the valuation of both, given the prevailing biases and perceptions. The paper emphasizes the need for strategic communication and presentation in the art market to navigate the complexities introduced by AI-created art. However, it focuses more on perception and valuation differences rather than user preferences and the detailed comparison between AI and human art on platforms like Pixiv.net, indicating a potential area for further exploration in my work.</p>
</section>
</section>
<section id="how-users-feel-about-generative-ai" class="level3">
<h3 class="anchored" data-anchor-id="how-users-feel-about-generative-ai">2.3 How users feel about generative AI</h3>
<section id="everyone-is-an-artist-a-study-on-user-experience-of-ai-based-painting-system" class="level4">
<h4 class="anchored" data-anchor-id="everyone-is-an-artist-a-study-on-user-experience-of-ai-based-painting-system">2.3.1 Everyone is an Artist? A study on user experience of AI-based painting system</h4>
<p>The paper, Everyone is an artist? A study on user experience of AI-based painting system<span class="citation" data-cites="EveryoneArtist"><a href="#ref-EveryoneArtist" role="doc-biblioref">[6]</a></span>, explores the integration of AI in the realm of painting, highlighting recent advances in AI-based Painting Systems (AIBPS). It delves into the technology acceptance model (TAM) to understand and predict users’ acceptance of AI technologies. It outlines a comprehensive model that includes external variables such as previous experience (PE), technical features (TF), hedonic motivation (HM), and perceived trust (PT), alongside core TAM variables like perceived usefulness (PU) and perceived ease of use (PEOU), to investigate their impact on users’ attitudes towards using AIBPS and their behavioral intention.</p>
<p>The discussion starts with an overview of AI in painting, noting that modern AIBPS leverages semantic analysis and deep learning algorithms to create new images from textual input. It mentions significant achievements, like the sale of the AI-generated portrait “Edmond de Belamy” and the variety of emerging neural network-based generative models. By discussing the TAM framework, the paper emphasizes its importance in evaluating new technology’s acceptance by measuring PU and PEOU, and how it has evolved to include additional external variables that influence these core perceptions. Research hypotheses are then developed to test the relationships between the external variables (PE, TF, HM, PT) and the core TAM constructs (PU, PEOU, ATT, BI). These hypotheses suggest that users’ previous experiences with AIBPS, the system’s technical features, users’ hedonic motivation, and their perceived trust in AIBPS will positively influence their perceived usefulness and ease of use of these systems, which in turn would affect their attitude towards using AIBPS and their behavioral intention to use it. The hypotheses aim to unravel the complexities of user acceptance of AIBPS, highlighting the role of both the intrinsic technology attributes and the users’ psychological motivations. This investigation is positioned as a significant contribution to the literature, offering insights for system developers and enterprises on how to enhance user engagement and promote the sustainable development of AI technologies in art and beyond.</p>
<p>The methods used in this paper outline the systematic approach undertaken by the authors to collect and analyze data for their study which is divided into three main parts: questionnaire design, participants and data collection, and demographic information of the participants.</p>
<p>The questionnaire was meticulously designed in three sections, starting with a brief introduction to AI painting and relevant images to provide context. The second part focused on gathering demographic information such as gender, age, educational background, frequency of use, and experience level with AIBPS. The main section aimed at understanding users’ willingness to utilize AIBPS, comprising 34 items across 8 variables to capture a wide range of attitudes and perceptions towards AIBPS. To ensure the questionnaire’s clarity and relevance, it underwent validation by five expert university professors with significant experience in teaching AI and art. Participants were incentivized with a small financial reward for their participation.</p>
<p>Data was collected online via a popular Chinese questionnaire platform, resulting in 568 responses. After a rigorous screening process that included removing invalid responses and controlling for typical technique bias, 528 valid questionnaires remained for analysis. This large sample size is deemed sufficient for Structural Equation Modeling (SEM) analysis, adhering to the requirement of a sample size greater than 200.</p>
<p>The demographic analysis of participants revealed a balanced gender distribution and a predominance of younger respondents, with a significant portion being undergraduates or holding higher educational qualifications. Most participants reported frequent use of AIBPS and had previous painting experience. Additionally, familiarity with different AI painting systems was also assessed, showing a high degree of awareness and usage among the participants.</p>
<p>The study on AI-Based Painting Systems (AIBPS) presents the findings from applying Structural Equation Modeling (SEM) to test the hypotheses formulated based on an extended Technology Acceptance Model (TAM). There are 13 hypotheses tested related to the influence of previous experience (PE), technical features (TF), hedonic motivation (HM), and perceived trust (PT) on perceived usefulness (PU) and perceived ease of use (PEOU). It also examined how PU, PEOU, and users’ attitudes (ATT) towards AIBPS affect their behavioral intentions (BI) to use the systems.</p>
<p>The results supported most of the hypotheses. Specifically, hedonic motivation (HM) and perceived trust (PT) positively influenced both PU and PEOU. This indicates that users’ enjoyment and trust in AIBPS significantly impact their perceptions of the system’s usefulness and ease of use. Contrary to expectations, PE and TF did not significantly impact PU, suggesting that users’ previous experiences with AIBPS and the technical aspects of the systems do not significantly influence their perceived usefulness. However, PE and TF positively influenced PEOU, indicating that these factors affect users’ perceptions of the ease of using AIBPS.</p>
<p>The SEM path analysis revealed the direct and indirect relationships among the variables. PU and PEOU had a significant positive impact on ATT and BI, confirming the core propositions of TAM that users’ perceptions of usefulness and ease of use influence their attitudes towards and intentions to use technology. The models the paper used demonstrated a good fit with the data, indicated by fit indices such as CMIN/DF, NFI, IFI, TLI, CFI, GFI, and RMSEA. This suggests that the extended TAM effectively captures the factors influencing user acceptance of AIBPS.</p>
<p>The study provides insights for AIBPS developers and researchers by highlighting the importance of hedonic motivation(HM) and perceived trust(PT) in influencing user acceptance. It suggests that enhancing the enjoyment and trustworthiness of AIBPS could improve users’ attitudes and intentions towards these systems and the usage of AI-based creative tools. Despite the positive impacts of HM and PT, previous experience (PE) and technical features (TF) did not significantly influence perceived usefulness (PU), suggesting that the interface and interaction design of AIBPS may already meet user expectations to a certain extent.</p>
<p>For future research, the paper acknowledges several limitations, including the regional focus on Chinese respondents, which may not capture global user perspectives. Future studies are encouraged to gather data from various countries to broaden the understanding of AIBPS acceptance. Additionally, the use of online questionnaires is noted as a potential constraint on understanding users’ attitudes comprehensively, suggesting that interviews or focus groups could provide deeper insights.</p>
</section>
<section id="understanding-the-impact-of-ai-generated-content-on-social-media-the-pixiv-case" class="level4">
<h4 class="anchored" data-anchor-id="understanding-the-impact-of-ai-generated-content-on-social-media-the-pixiv-case">2.3.2 Understanding the Impact of AI Generated Content on Social Media: The Pixiv Case</h4>
<p>The paper, Understanding the Impact of AI Generated Content on Social Media: The Pixiv Case<span class="citation" data-cites="AIGC_pixiv"><a href="#ref-AIGC_pixiv" role="doc-biblioref">[7]</a></span>, aims to explore the impact of AIGC on social media ecosystems by analyzing Pixiv, which uniquely hosts both humans and AIGC. Through an extensive dataset, the research investigates differences in content creation and consumption patterns between AIGC and human-generated content, aiming to understand AIGC’s influence on user engagement, community dynamics, and content themes. This inquiry is framed around three research questions focusing on the ecosystem’s temporal analysis, per-creator analysis, and per-content analysis, setting the stage for a comprehensive examination of AIGC’s role in reshaping social media dynamics.</p>
<p>In the ecosystem temporal analysis of this paper, we can draw several conclusions.</p>
<ul>
<li><p><strong>Impact on User Activity</strong>: The introduction of AIGC led to a 50% increase in the number of new artworks, but this surge in content creation did not correspond with an increase in views or user comments. This suggests that while more content was being produced, it did not necessarily engage the community more than before.</p></li>
<li><p><strong>Impact on Creators</strong>: The arrival of AIGC coincided with a growth in the number of new creators on Pixiv. However, there was a 4.3% decrease in newly registered creators of human-generated content, indicating a potential shift in the creator base towards AI-generated works. Despite this shift, the engagement of existing creators remained stable.</p></li>
<li><p><strong>Impact on Topics</strong>: Topics and subjects of artworks saw significant changes, with a decrease in diversity and a higher concentration of adult content and female characters. This shift suggests that AIGC may be influencing the types of content being created and shared within the Pixiv community.</p></li>
</ul>
<p>Overall, this analysis outlines how the integration of AIGC into Pixiv has led to notable shifts in content creation and consumption patterns, highlighting the complex interplay between technology and social media ecosystems.</p>
<p>The per-creator analysis delves into a comparative study of AI and human creators on Pixiv.net, focusing on aspects such as productivity, profile differences, and activities. We find AI creators can produce artwork more rapidly than human creators, with 55% of AI-generated artwork being uploaded on the same day as the previous one by the same creator, compared to 20% for human-generated artwork. Despite this efficiency, AI creators do not significantly upload more artworks than their human counterparts. AI creators not only show distinct demographic profiles, with a higher proportion identifying as male and working in IT-related fields than human creators but also are found to be less communicative, with a higher percentage of their artworks lacking captions compared to human-generated artworks. That leads to a noticeable trend among AI creators to use their posts for monetization more aggressively, with a higher percentage of AI-generated artworks including links to platforms like Fanbox, Patreon, and others. This analysis highlights the efficiency of AI creators in producing artworks and their distinct behavior in terms of communication and monetization, and also underscores the evolving dynamics between human and AI creators, reflecting on how AI is shaping content creation and creator interactions on the platform.</p>
<p>The per-content analysis investigates how consumer engagement with AIGC compares to engagement with human-created content on Pixiv.net. This analysis reveals distinct patterns in content consumption and interaction, focusing on views, bookmarks, popular themes, creators, and the nature of comments. AIGC sees different engagement levels compared to human-created content. While AI-generated works are more popular in the mid and lower popularity percentiles, they do not match the most popular human-created artworks in terms of views and bookmarks. This indicates that while AIGC plays a significant role in content consumption, it hasn’t yet eclipsed top human creators. Consumption of human-created content centers around the most popular creators and themes, receiving more views and bookmarks per capita. Conversely, the popularity of AIGC is more uniformly distributed, suggesting that AI-generated artworks attract a broader base of interest, albeit with less intensity than the peaks seen in human-created content. Analysis of comments reveals a clear pattern of interaction within groups; both human and AI creators are more likely to engage with content from their respective categories. Human-created artworks receive significantly more comments on average than AI-generated ones, highlighting a community preference or a higher willingness to engage with human creators. The findings from this analysis reveal how AIGC has integrated into the broader narrative of content creation and consumption, influencing user engagement, the distribution of popularity among artworks, and the dynamics of community interaction.</p>
<p>The paper provides a groundbreaking large-scale empirical analysis of AIGC’s impact on social media ecosystems, with a focus on Pixiv’s unique features. By comparing different platforms, the authors reveal how their insights could help in developing policies and algorithms for effectively managing AIGC. This approach not only underscores the originality and relevance of their findings but also contrasts with previous studies centered on user perceptions and the effects of AIGC on content authenticity and creator acknowledgment.</p>
<p>Throughout the paper, an increase in the production and consumption of AI-generated artworks is documented, alongside changing creator profiles and engagement patterns. Interestingly, these shifts have not significantly affected user interaction patterns, hinting at the Pixiv community’s preference for human-created content. However, the paper selectively addresses certain topics for analysis and stops short of exploring user preferences in detail, particularly in terms of how AI-generated artworks are received compared to those created by humans across varied themes. The study also expresses concerns about AIGC’s potential to sideline human creators, especially those new to the field, and its tendency to focus on specific themes like game-related content, which could impact the diversity of the platform. This nuanced exploration presents a complex picture of AIGC’s role in shaping online communities, emphasizing the need for careful consideration of its broader implications.</p>
</section>
</section>
</section>
<section id="methodology" class="level2">
<h2 class="anchored" data-anchor-id="methodology">3 Methodology</h2>
<section id="data-collection-and-processing" class="level3">
<h3 class="anchored" data-anchor-id="data-collection-and-processing">3.1 Data Collection and Processing</h3>
<section id="data-collection" class="level4">
<h4 class="anchored" data-anchor-id="data-collection">3.1.1 Data Collection</h4>
<p>In this study, we developed a web spider to crawl the dataset from pixiv.net, a prominent online platform where artists share their artwork. The website pixiv.net is widely recognized and highly regarded within the online art community, making it an ideal source for collecting data related to AI-generated artwork and user preferences.</p>
<p>The website pixiv.net provides a rich and diverse collection of artwork, including both AI-generated and hand-drawn creations. Artists from various backgrounds and genres contribute to the platform, producing a vast repository of artistic expressions.</p>
<p>The dataset I collected from the top list of pixiv.net/ranking covers a specific period, starting from November of the previous year and spanning a continuous influx of AI-generated works. It comprises essential information related to the artwork, such as tags, views, likes, bookmarks, and comments. Specifically, we focused on the Top 50 artworks or pictures each day, ensuring a comprehensive representation of the most popular and engaging content within the online art community.</p>
<p>By utilizing the data from pixiv.net, I’m able to examine the dynamic relationship between AI-generated artwork and user preferences over time. This allows us to gain valuable insights into the evolving landscape of the online art community and understand the factors that influence the rankings and preferences of AI-generated images compared to human-created artworks. #### 3.1.2 Data Description</p>
<p>We collected the data spanning from October 31, 2022, to May 15, 2023, from the top list of AI-generated and man-made image pages. After de-duplicating the same image pages which may appear at the top with different ranks and different days, we gathered the samples: Number of all samples: 14576 Number of samples of AI-generated Artworks: 8092 Number of samples of Hand-drawn or man-made Artworks: 6484</p>
</section>
</section>
<section id="topic-modeling-with-bertopic" class="level3">
<h3 class="anchored" data-anchor-id="topic-modeling-with-bertopic">3.2 Topic Modeling with BERTopic</h3>
<section id="introduction-1" class="level4">
<h4 class="anchored" data-anchor-id="introduction-1">3.2.1 Introduction</h4>
<p><strong>1. Topic Modeling in Understanding Online Art Preferences</strong></p>
<p>In the vast expanse of academic literature and digital content, the challenge of navigating and understanding thematic structures and trends has become increasingly complex. The discipline of natural language processing (NLP) offers a solution through topic modeling, a machine learning technique designed to identify and categorize thematic information from large datasets of text. Topic modeling leverages unsupervised learning algorithms to cluster documents into topics based on their content, facilitating a deeper understanding of the underlying themes without the need for predefined categories or labels <span class="citation" data-cites="BERTopicModeling"><a href="#ref-BERTopicModeling" role="doc-biblioref">[8]</a></span>.</p>
<p>The significance of topic modeling extends beyond academic research into practical applications within various industries, including the art market. As digital platforms like pixiv.net become central hubs for the exchange and appreciation of artwork, the ability to analyze and understand user preferences through the thematic analysis of art becomes crucial. This research focuses on employing topic modeling to explore the thematic landscape of AI-generated and human-created artwork shared online, aiming to uncover patterns and preferences within the user community.</p>
<p>Our dataset, drawn from pixiv.net is to apply topic modeling to the tags associated with these images to identify distinct topics and explore how these topics influence user preferences and rankings within the online painting community.</p>
<p>The study’s methodology is twofold. Initially, a deep learning model is utilized to perform topic modeling on the tags of images, aiming to derive distinct topics that characterize the difference between human-created and AI-generated artwork. This step is crucial for understanding the thematic structure that governs user preferences and rankings on the platform. Subsequent analysis will employ ordered choice models, specifically ordered logit and ordered probit methods, to examine the relationship between the categorized rankings of images and the topics derived from the initial phase. This approach allows for a quantitative analysis of how topics significantly influence the rankings and, by extension, user preferences for AI-generated versus human-created artwork.</p>
<p>By revealing the thematic preferences within the online art community, this research contributes to the broader understanding of how AI influences art consumption and appreciation. The findings are expected to offer valuable insights into the dynamics of user preferences in the digital painting market, shedding light on the potential for AI-generated artwork to complement or compete with human creativity.</p>
<p><strong>2. BERTopic for tag detecting</strong></p>
<p>BERTopic is an advanced unsupervised machine learning technique for clustering documents into topics. Unlike traditional clustering algorithms that rely solely on word frequency or pre-defined keyword matching, BERTopic utilizes the state-of-the-art BERT (Bidirectional Encoder Representations from Transformers) embeddings, coupled with c-TF-IDF (class-based Term Frequency-Inverse Document Frequency), to generate dense, easily interpretable topic clusters while preserving significant words within topic descriptions. This method facilitates the extraction of meaningful thematic structures from vast amounts of text data, providing a deeper understanding of content without the limitations of human-curated classifications <span class="citation" data-cites="BERTopicModeling"><a href="#ref-BERTopicModeling" role="doc-biblioref">[8]</a></span>.</p>
<p>The application of BERTopic is particularly advantageous in fields undergoing rapid expansion, such as natural language processing (NLP), where the volume of research outpaces the ability of traditional keyword-based searches to capture the full scope of developments. BERTopic’s use of the MiniLM-L6-v2 embedding model, known for its balance between performance and speed, allows for the efficient processing of large datasets, making it an ideal tool for parsing and understanding complex sentence structures within academic literature. This capability is critical for identifying emerging trends, research gaps, and connections between studies that might otherwise remain obscured in the deluge of published work.</p>
<p>In the context of this thesis, BERTopic offers a unique methodological advantage by allowing an in-depth analysis of user preferences in the online art market, specifically on platforms like pixiv.net. By applying BERTopic to the dataset collected from pixiv.net, encompassing both AI-generated and human-created artworks, this research can uncover the thematic distinctions that influence user rankings and preferences. This approach not only aids in validating the hypothesis that AI-generated artwork varies across different topics but also assists in exploring whether users exhibit distinct preferences for AI-generated versus human-created artworks.</p>
<p>The versatility and efficiency of BERTopic enable a nuanced exploration of these themes, offering insights into how the digital transformation of art affects user engagement and appreciation. The algorithm’s data-driven, unsupervised nature ensures an objective analysis, free from the biases inherent in manual topic classification, thus providing a robust foundation for investigating the dynamics of the online painting market. By leveraging BERTopic, this study aims to contribute significantly to the understanding of digital art trends and user preferences, informing strategies for content creation and curation within the burgeoning online art community.</p>
</section>
<section id="data-preparation-and-preprocessing" class="level4">
<h4 class="anchored" data-anchor-id="data-preparation-and-preprocessing">3.2.2 Data Preparation and Preprocessing</h4>
<p>The integrity and quality of data are paramount for the successful application of the BERTopic algorithm. The preprocessing phase aimed to refine the dataset to ensure the extracted topics are meaningful and representative of the underlying themes within the corpus of text documents derived from pixiv.net tags. The following steps were meticulously undertaken:</p>
<ol type="1">
<li><p><strong>Merging Tags</strong>: Each artwork’s associated tags were amalgamated into a single text document. This process converted the multi-tag structure of each artwork into a coherent text block, facilitating its analysis as a unified entity.</p></li>
<li><p><strong>Text Length Analysis</strong>: Given the varied length of text documents resulting from merged tags, an analysis and visualization of text length distribution were conducted. This step was crucial to identify outliers, particularly the shortest texts that might lack sufficient thematic depth for meaningful analysis. Subsequently, the shortest 1% of texts were removed from the dataset, justified by the visualization that highlighted their deviation from the typical text length, ensuring a more homogenous data set for analysis.</p></li>
<li><p><strong>Generation of N-grams</strong>: The creation of 2/3grams tokens played a significant role in capturing the context and enhancing the thematic structure identified by BERTopic. This process involved concatenating adjacent words into bi- and tri-gram tokens, thereby preserving phrase-level information that could be lost in single-word analysis. The rationale behind this step was to enrich the dataset with compound tokens that more accurately represent complex concepts or specific attributes of the artwork.</p></li>
<li><p><strong>Document Frequency Adjustment</strong>: To further refine the dataset, the document frequency of terms was analyzed and adjusted. This step involved the removal of extremely rare and overly common terms, which either appear in a negligible number of documents or nearly all documents, respectively. By adjusting the thresholds for term frequency, the dataset was optimized to exclude terms that contribute little to the uniqueness of topics or might otherwise dominate the topic modeling process due to their ubiquity.</p></li>
</ol>
<p>These preprocessing steps collectively aimed to balance the richness of the dataset with the pragmatic needs of computational analysis, setting a solid foundation for the subsequent application of the BERTopic algorithm. The careful consideration of text length, the strategic formation of n-grams, and the meticulous adjustment of document frequency underscored the methodology’s attentiveness to data quality, ensuring that the topics derived from the analysis are both robust and illuminating.</p>
</section>
<section id="bertopic-implementation" class="level4">
<h4 class="anchored" data-anchor-id="bertopic-implementation">3.2.3 BERTopic Implementation</h4>
<p>BERTopic Implementation outlines the rigorous process of applying the BERTopic algorithm to analyze and interpret the thematic structure of textual data. This process encompasses parameter tuning, data preprocessing, modeling, and evaluation of topic coherence. Below is a detailed description of each step, as implemented in the study:</p>
<p><strong>1. Parameter Tuning</strong></p>
<p>The first crucial step involved tuning various parameters to optimize the BERTopic model’s performance. Parameters such as n-gram range, document frequency thresholds (<code>max_df</code> and <code>min_df</code>), and UMAP’s <code>n_neighbors</code>, <code>n_components</code>, and <code>min_dist</code> were systematically varied in a grid search to identify the combination that yields the most coherent and meaningful topics. Specifically, the study explored:</p>
<ul>
<li><strong>N-gram range</strong>: The extent of n-gram tokens considered, set to 3, to capture more nuanced semantic relationships.</li>
<li><strong>Document Frequency (<code>max_df</code>, <code>min_df</code>)</strong>: Thresholds for filtering out terms that appear too frequently or infrequently across the dataset, thus refining the textual input for topic modeling.</li>
<li><strong>UMAP Parameters (<code>n_neighbors</code>, <code>n_components</code>, <code>min_dist</code>)</strong>: Settings for the dimensionality reduction step, crucial for managing the complexity and interpretability of the topic space.</li>
</ul>
<p>This exhaustive search was conducted while tracking computational performance and coherence scores, ensuring the selection of parameters that balance interpretability and computational efficiency.</p>
<p><strong>2. Modeling and Coherence Scoring</strong></p>
<p>With the optimized parameters, the BERTopic model was instantiated, incorporating UMAP for dimensionality reduction and HDBSCAN for clustering. The model’s ability to generate and categorize topics was then evaluated using coherence scores — UMass, UCI, and UCI-NPMI — calculated from the document-term matrix. These scores provide a quantitative measure of the topics’ semantic consistency, with higher values indicating more coherent and interpretable topics.</p>
<p><strong>3. Results and Visualization</strong></p>
<p>The implementation yielded a diverse range of topics, each characterized by a set of representative words. Coherence scores across different parameter settings were analyzed, and the top-performing configurations were identified. Visualization of coherence scores against <code>min_cluster_size</code> facilitated the selection of the optimal minimal topic size, further refining the model’s output.</p>
<p>The BERTopic model’s training was computationally intensive but resulted in a comprehensive topic structure that captures the thematic intricacies of the dataset. The <code>fit_transform</code> method, applied to the preprocessed texts, generated topics and their associated probabilities, signifying the model’s success in distilling complex textual data into distinct thematic clusters.</p>
<p><strong>4. Evaluation and Adjustment</strong></p>
<p>The final phase involved a detailed evaluation of the model’s output, with specific attention to the number of topics generated and their coherence scores. Adjustments were made iteratively to refine the topic quality, informed by the coherence scoring mechanisms and the qualitative assessment of topic relevance and separation.</p>
<p>This rigorous BERTopic implementation process underscores the methodological depth of the study, ensuring that the thematic analysis is both robust and insightful. The chosen parameters and modeling strategies were instrumental in uncovering the thematic landscape of the dataset, providing a solid foundation for subsequent analyses and interpretations.</p>
</section>
</section>
<section id="econometric-analysis-of-categorized-rankings" class="level3">
<h3 class="anchored" data-anchor-id="econometric-analysis-of-categorized-rankings">3.3 Econometric Analysis of Categorized Rankings</h3>
<section id="introduction-to-econometric-analysis" class="level4">
<h4 class="anchored" data-anchor-id="introduction-to-econometric-analysis">3.3.1 Introduction to Econometric Analysis</h4>
<p><strong>1. Ordered Choice Models and Their Application to Artwork Rankings</strong></p>
<p>Ordered choice models, encompassing both logit and probit models, have become indispensable tools in econometric analysis, especially for investigating outcomes that are naturally ordered but not necessarily linearly related to predictor variables <span class="citation" data-cites="UseLogitProbitModels"><a href="#ref-UseLogitProbitModels" role="doc-biblioref">[9]</a></span>. These models are particularly relevant for analyzing categorized ranks of artworks, where the ranking represents an ordered, continuous, and discrete variable. The nuanced nature of these rankings, reflecting preferences or qualities that cannot be precisely quantified on a linear scale, necessitates the use of models that can capture the ordinal essence of the data. In this context, ordered choice models allow for a sophisticated examination of how various independent variables influence the hierarchical positioning of artworks, offering insights into patterns and trends that underlie art valuation and appreciation.</p>
<p><strong>2. The Choice Between Ordered Logit and Ordered Probit Models</strong></p>
<p>The decision to utilize both ordered logit and ordered probit models stems from their suitability in handling ordinal data, which is characteristic of the categorized rankings of artworks. While these models are similar in their ability to manage ordered outcomes, they differ in the distributional assumptions they make about the error terms—logit models assume a logistic distribution, whereas probit models assume a normal distribution. This distinction is crucial as it affects the interpretation of the model’s coefficients and its fit to the data. As such, employing both models provides a robust framework for analyzing the relationships between independent variables and categorized rankings, ensuring that the conclusions drawn are not unduly influenced by the choice of distributional assumption. Furthermore, the use of the R programming language to implement these models offers a flexible and powerful environment for statistical analysis, enabling the exploration of model fit through various goodness-of-fit tests and the potential for model optimization based on test results <span class="citation" data-cites="UseLogitProbitModels"><a href="#ref-UseLogitProbitModels" role="doc-biblioref">[9]</a></span>.</p>
<p>By carefully selecting and employing these models, the analysis aims to uncover the intricate dynamics that govern the rankings of artworks, contributing valuable insights to the broader field of econometric research within the arts sector.</p>
</section>
<section id="model-implementation" class="level4">
<h4 class="anchored" data-anchor-id="model-implementation">3.3.2 Model Implementation</h4>
<p><strong>1. Use of the R Programming Language for Model Fitting</strong></p>
<p>In the pursuit of a rigorous analysis of categorized ranks within the art domain, we have leveraged the capabilities of the R programming language for model fitting. The choice of R is predicated on its extensive library of statistical tools and its adaptability to various types of econometric models, making it an ideal environment for our ordered choice model analysis. Specifically, we employed the logit function as the baseline model in our analysis. This function is particularly suited for estimating the probability of occurrence between ordered categorical outcomes, allowing us to effectively examine the relationships between independent variables and the categorized rankings of artworks. The utilization of the logit function within the R framework enables a nuanced exploration of the data, where the inherent properties of the categorized rank—being ordered, continuous, and discrete—are accounted for in the model estimation process.</p>
<p><strong>2. Introduction to Goodness-of-Fit Tests</strong></p>
<p>To assess the initial fit of our baseline ordered choice models, we conducted several Goodness-of-Fit tests, namely, the Hosmer-Lemeshow test, the Lipsitz test, and the Pulkstenis-Robinson test. These tests are instrumental in evaluating the concordance between observed outcomes and the predictions made by logistic regression models and, by extension, ordered choice models.</p>
<ul>
<li><p>The <strong>Hosmer-Lemeshow test</strong> assesses the model’s calibration by dividing the data into deciles based on predicted probabilities and then comparing the observed frequencies with the expected frequencies within these groups. A non-significant p-value from this test suggests that there is no significant difference between observed and model-predicted values, indicating a good model fit.</p></li>
<li><p>The <strong>Lipsitz test</strong> is tailored to the ordered nature of our data, providing a more nuanced evaluation by also dividing the data based on predicted probabilities and examining the alignment between observed and expected frequencies within these divisions. The calculation of a chi-square statistic helps to identify significant discrepancies, if any.</p></li>
<li><p>Similarly, the <strong>Pulkstenis-Robinson test</strong> utilizes a chi-square statistic to pinpoint areas where the model might not adequately capture the observed data patterns, offering another layer of scrutiny towards ensuring model adequacy.</p></li>
</ul>
<p>These Goodness-of-Fit tests serve as critical tools in our methodological toolkit, providing the initial validation needed to proceed with confidence in our model’s explanatory power. Based on the outcomes of these tests, further model optimization techniques are considered to refine the models and ensure they meet the stringent criteria for a robust analysis. By meticulously applying these tests, we aim to underpin our analysis with a solid foundation of statistical validity, thereby enhancing the reliability and interpretability of our findings in understanding the dynamics influencing the rankings of artworks.</p>
</section>
<section id="model-optimization-and-selection" class="level4">
<h4 class="anchored" data-anchor-id="model-optimization-and-selection">3.3.3 Model Optimization and Selection</h4>
<p><strong>1. Model Optimization Techniques</strong></p>
<ul>
<li><p><strong>Iterative Refinement for Model Accuracy</strong>: Discuss the iterative process of refining the model to enhance its fit with the dataset. This subsection should cover the initial steps taken after identifying unsatisfactory results from the Goodness-of-Fit tests, highlighting how these findings guide the optimization process.</p></li>
<li><p><strong>Enhancing Model Fit with Optimization Strategies</strong>: Explain various strategies for model optimization, such as adjusting model parameters, exploring alternative model specifications, and incorporating additional variables that may influence the outcome. Emphasize the importance of these techniques in improving the predictive accuracy and reliability of the model.</p></li>
</ul>
<p><strong>2. Brant Test for Model Type Selection</strong></p>
<ul>
<li><p><strong>Assessing Proportional Odds Assumption with the Brant Test</strong>: Elaborate on the Brant test’s methodology and its critical role in evaluating the proportional odds assumption underlying the ordered logit and probit models. This will involve a discussion on how the test compares coefficients across models to ensure the assumption holds.</p></li>
<li><p><strong>Impact of Model Selection on Interpretation and Performance</strong>: Discuss the implications of selecting either a logit or probit model based on the Brant test outcomes. This includes how the choice between these models affects the interpretation of the coefficients, model predictability, and overall performance in the context of the study’s objectives.</p></li>
</ul>
<p><strong>3. Significance of Model Selection</strong></p>
<ul>
<li><p><strong>Aligning Model Choice with Research Objectives</strong>: Address the importance of choosing the most appropriate model (logit vs.&nbsp;probit) in achieving the research objectives. This subsection should connect the methodological choices to the study’s overarching goals, such as uncovering the determinants of categorized rankings in the context of your dataset.</p></li>
<li><p><strong>Dataset Characteristics and Model Suitability</strong>: Explore how the unique characteristics of your dataset influence the selection of the ordered choice model. Discuss factors such as the distribution of the categorized rank, the nature of independent variables, and the dataset’s size and quality in guiding the choice between logit and probit models.</p></li>
</ul>
<p>These subsections will collectively provide a comprehensive overview of the methods and considerations involved in optimizing and selecting the most suitable ordered choice model for analyzing the relationships between independent variables and categorized rankings. The detailed examination of each aspect, from optimization techniques to the critical role of the Brant test and the importance of a deliberate model selection process, ensures a methodologically sound approach to achieving the study’s objectives.</p>
</section>
<section id="advanced-analysis-and-model-refinement" class="level4">
<h4 class="anchored" data-anchor-id="advanced-analysis-and-model-refinement">3.3.4 Advanced Analysis and Model Refinement</h4>
<p><strong>1. McFadden R2 Statistic</strong></p>
<ul>
<li><p><strong>Evaluating Explanatory Power</strong>: Detail the application of the McFadden R2 statistic in the context of ordered choice models, focusing on how this metric quantifies the models’ explanatory power. This subsection should explain how the McFadden R2 is calculated and why it is particularly suited for models where traditional R2 may not provide meaningful insights.</p></li>
<li><p><strong>Guiding Model Refinement</strong>: Discuss how the McFadden R2 statistic informs the refinement process, including the identification of models that adequately capture the underlying data structure. Highlight how improvements in the McFadden R2 statistic reflect on the model’s enhanced explanatory power and its implications for the research findings.</p></li>
</ul>
<p><strong>2. Likelihood Ratio Test and Nonlinear Relationships</strong></p>
<ul>
<li><p><strong>Model Comparison through Likelihood Ratio Test</strong>: Explain the use of the likelihood ratio test in comparing the fit of unrestricted and restricted models. This section should provide insights into how the test helps in determining the necessity of additional variables and the overall model complexity.</p></li>
<li><p><strong>Addressing Nonlinear Relationships</strong>: Discuss the exploration of nonlinear relationships within the dataset by incorporating power terms and other nonlinear transformations into the model. Illustrate how these adjustments can unveil more intricate dynamics between the independent variables and the ordered outcomes, enhancing the model’s accuracy and predictive capabilities.</p></li>
</ul>
<p><strong>3. General-to-Specific Method</strong></p>
<ul>
<li><p><strong>Iterative Variable Selection for Model Optimization</strong>: Describe the general-to-specific (GETS) method as an iterative process of variable selection aimed at refining the model’s accuracy and interpretability. This subsection should outline the steps taken to gradually reduce the model complexity by systematically eliminating statistically insignificant variables while preserving those with significant explanatory power.</p></li>
<li><p><strong>Uncovering Influential Factors</strong>: Focus on the ultimate goal of the GETS method in identifying the most influential variables determining the rankings. Emphasize how this process not only improves the model’s performance but also provides deeper insights into the factors that significantly impact the categorized rankings. This meticulous approach contributes valuable knowledge to the field of study, as documented by Campos, N. R. Ericsson, &amp; D. F. H. J. (2005) <span class="citation" data-cites="General-to-specific"><a href="#ref-General-to-specific" role="doc-biblioref">[10]</a></span> in their comprehensive overview and selected bibliography on general-to-specific modeling.</p></li>
</ul>
<p>These subsections collectively provide a framework for conducting advanced analyses and refining the ordered choice models used in the study. They illustrate the comprehensive approach taken to ensure the models’ robustness, from evaluating their explanatory power to exploring nonlinear relationships and optimizing variable selection. This meticulous process underscores the research’s commitment to producing accurate, reliable, and insightful results.</p>
</section>
</section>
<section id="conclusion-and-implications" class="level3">
<h3 class="anchored" data-anchor-id="conclusion-and-implications">3.4 Conclusion and Implications</h3>
<p>This chapter has delineated a comprehensive framework, encompassing data collection, topic modeling, and econometric analysis, to examine the intricate dynamics between AI-generated artwork and user preferences on the pixiv.net platform. This multifaceted approach has not only facilitated a deep dive into the thematic landscape of digital art but has also enabled a rigorous examination of the factors influencing artwork rankings, bridging the gap between qualitative thematic understanding and quantitative analysis.</p>
<p>The methodology began with a meticulous process of data collection and processing, capturing a wide array of AI-generated and human-created artworks. This foundation was pivotal in ensuring the study’s analyses were grounded in a rich and relevant dataset, reflective of the current state and diversity of digital art on a leading online platform.</p>
<p>The adoption of BERTopic for topic modeling marked a significant stride in uncovering thematic patterns and preferences within the online art community. This advanced NLP technique, through its unsupervised learning algorithm, offered nuanced insights into the thematic distinctions between AI-generated and human-created artworks. The analysis revealed not only the breadth of themes present in digital art but also highlighted the unique thematic signatures associated with AI-generated content.</p>
<p>The econometric analysis, leveraging ordered choice models, provided a quantitative lens through which to view the impact of thematic elements on artwork rankings. The use of both ordered logit and ordered probit models allowed for a nuanced understanding of the ordinal nature of artwork rankings, underscoring the complexity of user preferences and their implications for digital art valuation.</p>
<p>The rigorous process of model optimization and advanced analysis, including the implementation of McFadden R2 Statistic, likelihood ratio tests, and the general-to-specific (GETS) method, underscored the study’s commitment to methodological rigor. This approach not only enhanced the accuracy and interpretability of the models but also contributed to the literature on econometric analysis in the arts, as highlighted by the reference to Campos, N. R. Ericsson, &amp; D. F. H. J. (2005) <span class="citation" data-cites="General-to-specific"><a href="#ref-General-to-specific" role="doc-biblioref">[10]</a></span> on general-to-specific modeling.</p>
<p>This methodology chapter lays a solid foundation for the results that follow in the subsequent chapter, promising to enrich our understanding of the digital art landscape and the factors driving user engagement and preferences. By integrating topic modeling with econometric analysis, this study offers a holistic view of the digital art ecosystem, contributing valuable insights to both the academic field and practitioners within the digital art market.</p>
<p>Moreover, the methodological innovations and refinements introduced in this chapter have broader implications for econometric research, particularly in studies involving complex, ordinal datasets. The application of advanced NLP techniques like BERTopic, coupled with sophisticated econometric models, sets a precedent for future research at the intersection of digital humanities and quantitative analysis.</p>
<p>The chapter methodology employed in this thesis represents a significant leap forward in the study of digital art, offering a robust framework for understanding the interplay between thematic content, technological innovation, and user preferences. The insights gleaned from this approach not only illuminate the current landscape of digital art but also pave the way for future investigations into the evolving relationship between art, technology, and society.</p>
</section>
</section>
<section id="results" class="level2">
<h2 class="anchored" data-anchor-id="results">4 Results</h2>

</section>
<section id="conclusion-and-future-prospects" class="level2">
<h2 class="anchored" data-anchor-id="conclusion-and-future-prospects">5 Conclusion and Future Prospects</h2>
<section id="conclusion" class="level3">
<h3 class="anchored" data-anchor-id="conclusion">5.1 Conclusion</h3>
</section>
<section id="future-prospects" class="level3">
<h3 class="anchored" data-anchor-id="future-prospects">5.2 Future Prospects</h3>

</section>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<div id="refs" class="references csl-bib-body" role="list">
<div id="ref-AIThemedCryptocurrencies" class="csl-entry" role="listitem">
<div class="csl-left-margin">[1] </div><div class="csl-right-inline">L. Ante and E. Demir, <span>“The ChatGPT effect on AI-themed cryptocurrencies,”</span> <em>SSRN</em>, 2023, Available: <a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4350557">papers.ssrn.com/sol3/papers.cfm?abstract_id=4350557</a></div>
</div>
<div id="ref-CyberTurnArt" class="csl-entry" role="listitem">
<div class="csl-left-margin">[2] </div><div class="csl-right-inline">E. Sidorova, <span>“The cyber turn of the contemporary art market,”</span> <em>Art Markets and Digital Histories</em>, Jul. 2019, Available: <a href="https://doi.org/10.3390/arts8030084">https://doi.org/10.3390/arts8030084</a></div>
</div>
<div id="ref-ArtisticReflection" class="csl-entry" role="listitem">
<div class="csl-left-margin">[3] </div><div class="csl-right-inline">X. Liu, <span>“Artistic reflection on artificial intelligence digital painting,”</span> <em>Journal of Physics</em>, 2020, Available: <a href="https://doi:10.1088/1742-6596/1648/3/032125">doi:10.1088/1742-6596/1648/3/032125</a></div>
</div>
<div id="ref-Pigments2Pixels" class="csl-entry" role="listitem">
<div class="csl-left-margin">[4] </div><div class="csl-right-inline">Y. Sun, C.-H. Yang, Y. Lyu, and R. Lin, <span>“From pigments to pixels: A comparison of human and AI painting,”</span> <em>Applied Sciences</em>, Apr. 2022, Available: <a href="https://doi.org/10.3390/app12083724">doi.org/10.3390/app12083724</a></div>
</div>
<div id="ref-AirtistOrCounterfeiter" class="csl-entry" role="listitem">
<div class="csl-left-margin">[5] </div><div class="csl-right-inline">P. Fortuna and A. Modliński, <span>“A(i)rtist or counterfeiter? Artificial intelligence as (d)evaluating factor on the art market,”</span> <em>The Journal of Arts Management, Law, and Society</em>, vol. 51, pp. 188–201, Mar. 2021, Available: <a href="https://doi.org/10.1080/10632921.2021.1887032">https://doi.org/10.1080/10632921.2021.1887032</a></div>
</div>
<div id="ref-EveryoneArtist" class="csl-entry" role="listitem">
<div class="csl-left-margin">[6] </div><div class="csl-right-inline">J. Xu, X. Zhang, H. Li, C. Yoo, and Y. Pan, <span>“Everyone is an artist? A study on user experience of AI-based painting system,”</span> <em>preprints</em>, Apr. 2023, Available: <a href="https://doi:10.20944/preprints202304.0593.v1">doi:10.20944/preprints202304.0593.v1</a></div>
</div>
<div id="ref-AIGC_pixiv" class="csl-entry" role="listitem">
<div class="csl-left-margin">[7] </div><div class="csl-right-inline">Y. Wei and G. Tyson, <span>“Understanding the impact of AI generated content on social media: The pixiv case,”</span> <em>arXiv</em>, Feb. 2024, Available: <a href="https://doi.org/10.48550/arXiv.2402.18463">doi.org/10.48550/arXiv.2402.18463</a></div>
</div>
<div id="ref-BERTopicModeling" class="csl-entry" role="listitem">
<div class="csl-left-margin">[8] </div><div class="csl-right-inline">Samsir, R. S. Saragih, S. Subagio, R. Aditya, and R. Watrianthos, <span>“BERTopic modeling of natural language processing abstracts: Thematic structure and trajectory,”</span> <em>JURNAL MEDIA INFORMATIKA BUDIDARMA</em>, vol. 7, pp. 1514–1520, Jul. 2023, Available: <a href="https://ejurnal.stmik-budidarma.ac.id/index.php/mib">https://ejurnal.stmik-budidarma.ac.id/index.php/mib</a></div>
</div>
<div id="ref-UseLogitProbitModels" class="csl-entry" role="listitem">
<div class="csl-left-margin">[9] </div><div class="csl-right-inline">G. Hoetker, <span>“The use of logit and probit models in strategic management research: Critical issues,”</span> <em>Strategic Management Journal</em>, vol. 28, pp. 331–343, Jul. 2007, Available: <a href="https://doi.org/10.1002/smj.582">https://doi.org/10.1002/smj.582</a></div>
</div>
<div id="ref-General-to-specific" class="csl-entry" role="listitem">
<div class="csl-left-margin">[10] </div><div class="csl-right-inline">J. Campos, N. R. Ericsson, and D. F. Hendry, <span>“General-to-specific modeling: An overview and selected bibliography, board of governors of the federal reserve system,”</span> <em>International Finance Discussion Papers</em>, no. 838, Aug. 2005.</div>
</div>
</div>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>